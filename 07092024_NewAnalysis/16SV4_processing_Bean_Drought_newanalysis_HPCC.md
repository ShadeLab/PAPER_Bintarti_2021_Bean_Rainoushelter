# Raw 16S-V4 Sequence Analysis for Bean Drought Field Experiment 

## Analysis of 16S Miseq Data
Here we perform processing raw sequences 16S data from the bean drought field experiment that have been generated by Argonne National Laboratory (USA). This data are generated by re-sequencing samples due to lack of sequencing depth from the previous sequencing process. The sequence data had been demultiplexed by Argonne and we received the demultiplexed paired-end reads to proceed with. Raw-sequence data processing is conducted using DADA2 implemented in the qiime2 and the analysis is perfomed on MSU HPCC. (demultiplexing: to determine which sample each read came from)

## Raw-sequence data are stored:
"/mnt/research/ShadeLab/Sequence/raw_sequence/Bean_rainoutshelter/2022"

## Versions in the HPCC:
conda = conda 24.5.0
Follow the qiime2 installation here: https://docs.qiime2.org/2024.5/install/native/

raw sequence data stored on HPCC together with Abby Sulesky's data:
/mnt/research/ShadeLab/Sequence/raw_sequence/Bean_heritability

Moved/copy raw sequences to the working space:
/mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis/Argonne_demux_June_2023_AS_copy


## All analysis results are stored on hpcc: 

"/mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis"

# Part I: Quality control and denoising using DADA2


## 1) Determine the truncation parameters with FIGARO
```
## Prior to denoising  using DADA2, you need to investigate your Q-score distribution of your reads to determine the 'trunc-len-f' and 'trunc-len-r' values. The goal is to trim the reads/sequences by removing as much of the lower quality portions of the reads as possible and still leave enough overlap so that merging of the forward and reverse reads can be optimized.

##run the figaro activation and tutorial from John Quensen's website (http://john-quensen.com/tutorials/figaro/) and the reference can be found here: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7424690/

######## deactivate qiime2 before use figaro because figaro need miniconda3, while qiime2 environment need anaconda2 #############
conda deactivate


## Note:
Use 'rename' to make files match illumina naming convention: SampleName_S1_L001_R1_001.fastq

Example:
>for file in Mock6*.fastq; do     
mv "$file" "${file/_Fina/}"; 
done

this code takes all files that end in *fastq and remove "Fina"

had to run various lines to remove extra underscores in sample IDs and add L001 to all:
>  for f in *.fastq; do mv "$f" "$(echo "$f" | sed s/_R/_L001_R/)"; done


## activate figaro

cd
source ~/miniconda3/etc/profile.d/conda.sh
conda activate figaro

## run command

mkdir figaro

python /mnt/home/bintarti/figaro/figaro/figaro.py -i /mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis/Argonne_demux_June_2023_AS_copy/ -o /mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis/figaro/ -f 1 -r 1 -a 253 -F illumina

> nano trimParameters.json

{
        "trimPosition": [
            192,
            83
        ],
        "maxExpectedError": [
            2,
            2
        ],
        "readRetentionPercent": 95.52,
        "score": 93.5163950187194
    },


## Based on the figaro result, the recommended forward truncation position is 192 and the recommended reverse truncation position is 83. After trimming and truncation, the expected number of errors in the forward and reverse read is 2. By using this parameters should result in merging 95.52% of the reads.

## deactivate figaro
conda deactivate
```

## 2. Importing paired-end reads (Casava 1.8 paired-end demultiplexed fastq)

```
Note: Apparently, you cannot use python 3 with qiime2 due to incompatibility issue with the "cffi" package (Exception: Version mismatch: this is the 'cffi' package version 1.15.1, located in '/opt/software-current/2023.06/x86_64/generic/software/Python-bundle-PyPI/2023.06-GCCcore-12.3.0/lib/python3.11/site-packages/cffi/api.py'.  When we import the top-level '_cffi_backend' extension module, we get version 1.16.0, located in '/mnt/home/bintarti/miniconda3/envs/qiime2-amplicon-2024.5/lib/python3.9/site-packages/_cffi_backend.cpython-39-x86_64-linux-gnu.so'.  The two versions should be equal; check your installation.)

So, if you have installed miniconda3 that's fine. Just do "conda create -n py2 python=2.7"
and activate by:

conda activate py2

## then re-install later version of qiime2, decided to downgrade qiime2-amplicon-2024.5 --> qiime2-2023.2

##  activate qiime2

conda activate qiime2-2023.2

## have to zip the fastqs with gzip to .gz:

gzip *.fastq


## submit job to MSU HPCC for importing fastq.gz to .qza

__________________________________________________________________________________________________________
#!/bin/bash -login
########## SBATCH Lines for Resource Request ##########

#SBATCH --time=3:59:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=64G
#SBATCH --job-name import_data
#SBATCH -A shade-cole-bonito
#SBATCH --mail-user=bintarti@msu.edu
#SBATCH --mail-type=BEGIN,END

########## Command Lines for Job Running ##########

conda activate qiime2-2023.2

cd /mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis

qiime tools import \
  --type 'SampleData[PairedEndSequencesWithQuality]' \
  --input-path Argonne_demux_June_2023_AS_copy \
  --input-format CasavaOneEightSingleLanePerSampleDirFmt \
  --output-path import_qiime2/Argonne_demux_June_2023_AS_copy.qza

echo -e "\n `sacct -u bintarti -j $SLURM_JOB_ID --format=JobID,JobName,Start,End,Elapsed,NCPUS,ReqMem` \n"
scontrol show job $SLURM_JOB_I
__________________________________________________________________________________________________________

# command to submit the job

sbatch denoise_job.sb

## successfully imported as .qza file (Argonne_demux_June_2023_AS_copy.qza)
```

## 3) Denoise with DADA2 implemented in qiime2 (denoise paired-end requires unmerged, paired-end reads (i.e. both forward and reverse)

```
Note: Whether or not you need to merge reads depends on how you plan to cluster or denoise your sequences into amplicon sequence variants (ASVs) or operational taxonomic units (OTUs). If you plan to use deblur or OTU clustering methods next, join your sequences now. If you plan to use dada2 to denoise your sequences, do not merge — dada2 performs read merging automatically after denoising each sequence.
Denoising requires little data preparation. Both DADA2 and deblur perform quality filtering, denoising, and chimera removal, so you shouldn’t need to perform any quality screening prior to running them. 
Q-score based filtering is built in to DADA2, so doing this quality-filter step prior to denoising with DADA2 is unnecessary.
Reads shorter than the truncation length are discarded and reads longer are truncated at that position.


## It can take a while if you have many samples. Thus it's better to submit as a job on MSU HPCC. Path to the job file:
/mnt/scratch/bintarti/20220706_Bean_rainout_analysis/denoise_dada2.sb
## command for denoising using DADA2 (This performs quality filtering, chimera checking, and paired- end read joining)


## submit job to MSU HPCC for denoising

__________________________________________________________________________________________________________
#!/bin/bash -login
########## SBATCH Lines for Resource Request ##########

#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=100G
#SBATCH --job-name denoise_dada2_job
#SBATCH -A shade-cole-bonito
#SBATCH --mail-user=bintarti@msu.edu
#SBATCH --mail-type=BEGIN,END

########## Command Lines for Job Running ##########

conda activate qiime2-2023.2

cd /mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis

qiime dada2 denoise-paired \
--i-demultiplexed-seqs import_qiime2/Argonne_demux_June_2023_AS_copy.qza \
--p-trunc-len-f 192 \
--p-trunc-len-r 83 \
--o-table denoise_outputs2/table.qza \
--o-representative-sequences denoise_outputs2/rep-seqs.qza \
--o-denoising-stats denoise_outputs2/denoising-stats.qza \
--verbose \
--p-n-threads 10


qiime metadata tabulate --m-input-file denoise_outputs2/denoising-stats.qza --o-visualization denoise_outputs2/denoising-stats.qzv
qiime feature-table summarize --i-table denoise_outputs2/table.qza --o-visualization denoise_outputs2/table.qzv
qiime feature-table tabulate-seqs --i-data denoise_outputs2/rep-seqs.qza --o-visualization denoise_outputs2/rep-seqs.qzv


echo -e "\n `sacct -u bintarti -j $SLURM_JOB_ID --format=JobID,JobName,Start,End,Elapsed,NCPUS,ReqMem` \n"
scontrol show job $SLURM_JOB_I
__________________________________________________________________________________________________________

sbatch "denoise_job_dada2.sb"


#Note: I've encountered this warning when running denoise on HPCC > package ‘optparse’ was built under R version 4.2.3 
R version 4.2.2 (2022-10-31) --> You can disregard that warning message - this type of warning appears when you load a package that was built under a version of R different to the one you are running. Unless you run into any R related errors while running DADA2 (or any other package that uses R), this is typically fine to ignore (https://forum.qiime2.org/t/warning-message-during-dada2-analysis/28685)
```

## 4) Assign taxonomy using pre-trained (515F-806R) SILVA 138 reference database downloaded from the QIIME2 website: https://docs.qiime2.org/2021.4/data-resources/. Reference: Bokulich et al. 2018 (https://doi.org/10.1186/s40168-018-0470-z, http://doi.org/10.5281/zenodo.3891931). Classify-sklearn is a machine learning based classification method with a Naive Bayes classifier.
```
##  Download this file from the QIIME2 website:

silva-138-99-515-806-nb-classifier.qza 

# check if you download everything correctly:

shasum -a 256 silva-138-99-515-806-nb-classifier.qza 


## submit job to MSU HPCC for taxonomy assignment
__________________________________________________________________________________________________________
#!/bin/bash -login
########## SBATCH Lines for Resource Request ##########

#SBATCH --time=24:00:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=32
#SBATCH --mem=100G
#SBATCH --job-name tax_assign_job
#SBATCH -A shade-cole-bonito
#SBATCH --mail-user=bintarti@msu.edu
#SBATCH --mail-type=BEGIN,END

########## Command Lines for Job Running ##########

conda activate qiime2-2023.2

cd /mnt/research/ShadeLab/WorkingSpace/Bintarti/Bean_rainoutshelter/20240107_newanalysis

qiime feature-classifier classify-sklearn \
  --i-classifier silva-138-99-515-806-nb-classifier.qza \
  --i-reads denoise_outputs2/rep-seqs.qza \
  --o-classification tax_assign_outputs/taxonomy.qza \
  --p-confidence .8 \
  --p-n-jobs 10

qiime metadata tabulate --m-input-file tax_assign_outputs/taxonomy.qza --o-visualization tax_assign_outputs/taxonomy.qzv

echo -e "\n `sacct -u bintarti -j $SLURM_JOB_ID --format=JobID,JobName,Start,End,Elapsed,NCPUS,ReqMem` \n"
scontrol show job $SLURM_JOB_I
__________________________________________________________________________________________________________

sbatch "tax_assign_job.sb"

# for tax assignment with default confidence (=0.7)
qiime feature-classifier classify-sklearn \
  --i-classifier silva-138-99-515-806-nb-classifier.qza \
  --i-reads denoise_outputs2/rep-seqs.qza \
  --o-classification tax_07/tax_assign_out_07/taxonomy_07.qza \
  --p-confidence .7 \
  --p-n-jobs 10
```

## 5) Filter non-bacteria/archcd from tables and sequences
```
# from OTU table

qiime taxa filter-table \
  --i-table denoise_outputs2/table.qza \
  --i-taxonomy tax_assign_outputs/taxonomy.qza \
  --p-exclude unassigned,mitochondria,chloroplast \
  --o-filtered-table plant_filtered_outputs/ASVtable-no-mito_et_chloro.qza

# from rep sequences

qiime taxa filter-seqs \
  --i-sequences denoise_outputs2/rep-seqs.qza \
  --i-taxonomy tax_assign_outputs/taxonomy.qza \
  --p-exclude unassigned,mitochondria,chloroplast \
  --o-filtered-sequences plant_filtered_outputs/rep-seqs-no-mito_et_chloro.qza

### output ###
ASVtable-no-mitoch-no-chloro.qza
rep-seqs-no-mitoch-no-chloro.qza

## create visualization files

qiime feature-table summarize --i-table plant_filtered_outputs/ASVtable-no-mito_et_chloro.qza --o-visualization plant_filtered_outputs/ASVtable-no-mito_et_chloro.qzv
qiime feature-table tabulate-seqs --i-data plant_filtered_outputs/rep-seqs-no-mito_et_chloro.qza --o-visualization plant_filtered_outputs/rep-seqs-no-mito_et_chloro.qzv

```

## 6) Align the representative sequences and construct a phylogenetic tree from the alignment using MAFFT (Multiple Alignment using Fast Fourier Transform)
```
qiime phylogeny align-to-tree-mafft-fasttree \
--i-sequences plant_filtered_outputs/rep-seqs-no-mito_et_chloro.qza \
--o-alignment align_outputs/aligned-rep-seqs-no-mito_et_chloro.qza \
--o-masked-alignment align_outputs/masked-aligned-rep-seqs-no-mito_et_chloro.qza \
--o-tree align_outputs/unrooted-tree.qza \
--o-rooted-tree align_outputs/rooted-tree.qza

### output ###
aligned-rep-seqs-no-mito_et_chloro.qza
masked-aligned-rep-seqs-nno-mito_et_chloro.qza
unrooted-tree.qza
rooted-tree.qza

qiime tools export \
  --input-path align_outputs/unrooted-tree.qza \
  --output-path align_outputs/exported-unrooted-tree

qiime tools export \
  --input-path align_outputs/rooted-tree.qza \
  --output-path align_outputs/exported-rooted-tree
```

## 7) Export table to biom
```
mkdir export_file

cp denoise_outputs2/table.qza export_file/
cp plant_filtered_outputs/ASVtable-no-mito_et_chloro.qza export_file/
cp tax_assign_outputs/taxonomy.qza export_file/ 

qiime tools export \
    --input-path export_file/table.qza \
    --output-path Biom/

mv Biom/feature-table.biom Biom/ASVtable_unfiltered.biom

qiime tools export \
    --input-path export_file/ASVtable-no-mito_et_chloro.qza \
    --output-path Biom/

mv Biom/feature-table.biom Biom/ASVtable-no-mito_et_chloro.biom

biom convert -i Biom/ASVtable_unfiltered.biom -o Biom/ASVtable_unfiltered.tsv --to-tsv

biom convert -i Biom/ASVtable-no-mito_et_chloro.biom -o Biom/ASVtable-no-mito_et_chloro.tsv --to-tsv

qiime tools export --input-path export_file/taxonomy_07.qza --output-path exported_tax

cp exported_tax/taxonomy_07.tsv Biom/biom-taxonomy_07.tsv
```

## 8) Add taxonomy to the ASV table
```
biom add-metadata -i Biom/ASVtable_unfiltered.biom -o Biom/ASVtable_unfiltered_tax.biom --observation-metadata-fp=Biom/biom-taxonomy.tsv --sc-separated=taxonomy --observation-header=OTUID,taxonomy,confidence

biom add-metadata -i Biom/ASVtable-no-mito_et_chloro.biom -o Biom/ASVtable-no-mito_et_chloro_tax.biom --observation-metadata-fp=Biom/biom-taxonomy.tsv --sc-separated=taxonomy --observation-header=OTUID,taxonomy,confidence

biom convert -i Biom/ASVtable_unfiltered_tax.biom -o Biom/ASVtable_unfiltered_tax.tsv --header-key taxonomy --to-tsv

biom convert -i Biom/ASVtable-no-mito_et_chloro_tax.biom -o Biom/ASVtable-no-mito_et_chloro_tax.tsv --header-key taxonomy --to-tsv
```







































